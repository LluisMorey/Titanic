# âš“ Titanic Passengers Classification - Machine Learning

![Screenshot](screenshot1.jpeg)

## The Challenge

The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered â€œunsinkableâ€ RMS Titanic sank after colliding with an iceberg. Unfortunately, there werenâ€™t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

# ğŸ¯ Goal

The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.

# ğŸ’¡ About the data

â€¢ Pclass : Ticket class (1st = Upper, 2nd = Middle and 3rd = Lower)

â€¢ Sex: Gender (Male and Female)

â€¢ Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

â€¢ SibSp: Number of siblings / spouses aboard the Titanic.

â€¢ Parch: Number of parents / children aboard the Titanic.

â€¢ Ticket: Ticket number.

â€¢ Fare: Passenger fare.

â€¢ Cabin: Cabin number.

â€¢ Embarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)


# ğŸŒ¿ Gradient Boosting Trees Model and its Architecture

![Screenshot](screenshot2.png)

The model was developed and trained using Sklearn, Feature_Engine and Imblearn. Gradient Boosting Trees has been the algorithm which has performed best in this classification problem. Gradient boosting works by building simpler (weak) prediction models sequentially where each model tries to predict the error left over by the previous model.

## Best results : 77.272 % accuracy


# ğŸš€ Future Work

â€¢ Feature Engineering with Cabin Variables

â€¢ Feature Engineering with SibSp and Parch

â€¢ Model Tunning other models such as Logistic Regression, Stochastic Gradient Decent, k-Nearest Neighbor and Linear SVC

â€¢ Compare the notebook with other Kaggle users to see what approach they have follow.
